{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93877b8f",
   "metadata": {},
   "source": [
    "## ARIMA Setup\n",
    "\n",
    "The purpose of this notebook is to read in the main catdef data from the pickle file, pre-process it, and stage it for processing by *auto.arima* in R."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49667c5a",
   "metadata": {},
   "source": [
    "### STEP 1.  Load Modules and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbfc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41074ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "objects = []\n",
    "with (open(\"all_data.pkl\", \"rb\")) as openfile:\n",
    "    while True:\n",
    "        try:\n",
    "            objects.append(pickle.load(openfile))\n",
    "        except EOFError:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141c5630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(str(len(objects)))\n",
    "pdata=objects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ca80a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af30012f",
   "metadata": {},
   "source": [
    "### STEP 2.  Extract Catdef for ARIMA\n",
    "\n",
    "Having loaded the data, begin to stage it.  Before staging it, load the region mask file.  The region mask file indicates for each grid location, the region to which it belongs.  This information can be used to know which grid locations do *not* belong to any region so that no data for these irrelevant regions is written or so as to eliminate any chance of its analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70ed7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_mask_file=\"Region_mask.csv\"\n",
    "region_data=list()\n",
    "with open(region_mask_file,'r') as reader:\n",
    "    temp_list=None\n",
    "    for line in reader:\n",
    "        pieces=line.split(',')\n",
    "        temp_list=[int(p.strip()) for p in pieces]\n",
    "        region_data.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a3f4742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region data has 33 rows.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Region data has {len(region_data)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a680374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0 in region data has 37 columns.\n",
      "Row 1 in region data has 37 columns.\n",
      "Row 2 in region data has 37 columns.\n",
      "Row 3 in region data has 37 columns.\n",
      "Row 4 in region data has 37 columns.\n",
      "Row 5 in region data has 37 columns.\n",
      "Row 6 in region data has 37 columns.\n",
      "Row 7 in region data has 37 columns.\n",
      "Row 8 in region data has 37 columns.\n",
      "Row 9 in region data has 37 columns.\n",
      "Row 10 in region data has 37 columns.\n",
      "Row 11 in region data has 37 columns.\n",
      "Row 12 in region data has 37 columns.\n",
      "Row 13 in region data has 37 columns.\n",
      "Row 14 in region data has 37 columns.\n",
      "Row 15 in region data has 37 columns.\n",
      "Row 16 in region data has 37 columns.\n",
      "Row 17 in region data has 37 columns.\n",
      "Row 18 in region data has 37 columns.\n",
      "Row 19 in region data has 37 columns.\n",
      "Row 20 in region data has 37 columns.\n",
      "Row 21 in region data has 37 columns.\n",
      "Row 22 in region data has 37 columns.\n",
      "Row 23 in region data has 37 columns.\n",
      "Row 24 in region data has 37 columns.\n",
      "Row 25 in region data has 37 columns.\n",
      "Row 26 in region data has 37 columns.\n",
      "Row 27 in region data has 37 columns.\n",
      "Row 28 in region data has 37 columns.\n",
      "Row 29 in region data has 37 columns.\n",
      "Row 30 in region data has 37 columns.\n",
      "Row 31 in region data has 37 columns.\n",
      "Row 32 in region data has 37 columns.\n"
     ]
    }
   ],
   "source": [
    "for row_idx in range(len(region_data)):\n",
    "    row=region_data[row_idx]\n",
    "    print(f\"Row {row_idx} in region data has {len(row)} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b160f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "extraction_dir=\"catdef_extraction_6mo\"\n",
    "if(not(os.path.exists(extraction_dir))):\n",
    "    os.system(\"mkdir -v \"+extraction_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7eb5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "catdef_dims=pdata.apply(lambda row:row['catdef'].shape,axis=1).tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40c28d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33, 37)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdef_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1728a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row is 0 of 33 ...\n",
      "row is 1 of 33 ...\n",
      "row is 2 of 33 ...\n",
      "row is 3 of 33 ...\n",
      "row is 4 of 33 ...\n",
      "row is 5 of 33 ...\n",
      "row is 6 of 33 ...\n",
      "row is 7 of 33 ...\n",
      "row is 8 of 33 ...\n",
      "row is 9 of 33 ...\n",
      "row is 10 of 33 ...\n",
      "row is 11 of 33 ...\n",
      "row is 12 of 33 ...\n",
      "row is 13 of 33 ...\n",
      "row is 14 of 33 ...\n",
      "row is 15 of 33 ...\n",
      "row is 16 of 33 ...\n",
      "row is 17 of 33 ...\n",
      "row is 18 of 33 ...\n",
      "row is 19 of 33 ...\n",
      "row is 20 of 33 ...\n",
      "row is 21 of 33 ...\n",
      "row is 22 of 33 ...\n",
      "row is 23 of 33 ...\n",
      "row is 24 of 33 ...\n",
      "row is 25 of 33 ...\n",
      "row is 26 of 33 ...\n",
      "row is 27 of 33 ...\n",
      "row is 28 of 33 ...\n",
      "row is 29 of 33 ...\n",
      "row is 30 of 33 ...\n",
      "row is 31 of 33 ...\n",
      "row is 32 of 33 ...\n",
      "nan_counts_obs {168}\n",
      "num nan data points : 363\n",
      "Num non-CA data points 418\n",
      "Num data files written 440\n"
     ]
    }
   ],
   "source": [
    "nan_counts_obs=set()\n",
    "num_nan_dps=0\n",
    "num_outside_ca_dps=0\n",
    "num_data_written=0\n",
    "for row_idx in range(catdef_dims[0]):\n",
    "    print(f\"row is {row_idx} of {catdef_dims[0]} ...\")\n",
    "    for col_idx in range(catdef_dims[1]):\n",
    "        the_header=f\"catdef_row.{row_idx}.col.{col_idx}\"\n",
    "        csv_target=f\"{extraction_dir}/catdef.{the_header}.csv\"\n",
    "        the_data=pdata.apply(lambda row:row['catdef'].item((row_idx,col_idx)),axis=1).tolist()\n",
    "        nan_data=[d for d in the_data if np.isnan(d)]\n",
    "        num_nans=len(nan_data)\n",
    "        if(num_nans>0):\n",
    "            nan_counts_obs.add(num_nans)\n",
    "            num_nan_dps=num_nan_dps+1\n",
    "            #this means that any nan data is never written!\n",
    "            continue\n",
    "        region_id=region_data[row_idx][col_idx]\n",
    "        if(region_id==0):\n",
    "            num_outside_ca_dps=num_outside_ca_dps+1\n",
    "            continue\n",
    "        temp_df=pd.DataFrame.from_dict({the_header:the_data})\n",
    "        temp_df.to_csv(csv_target,index=False)\n",
    "        num_data_written=num_data_written+1\n",
    "        #break\n",
    "    #break\n",
    "print(f\"nan_counts_obs {nan_counts_obs}\")\n",
    "print(f\"num nan data points : {num_nan_dps}\")\n",
    "print(f\"Num non-CA data points {num_outside_ca_dps}\")\n",
    "print(f\"Num data files written {num_data_written}\")\n",
    "#this shows that any series is either all-data or all-nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1713540",
   "metadata": {},
   "source": [
    "### STEP 3.  Duplication for Multiple ARIMAs and Execution\n",
    "\n",
    "After data is staged, it *must* be duplicated if ARIMA will be run with and without 'lambda=\"auto\"'.\n",
    "\n",
    "To duplicate it run the commands\n",
    "\n",
    "```\n",
    "mkdir -v catdef_extraction_6mo_lambda\n",
    "cp -v catdef_extraction_6mo/* catdef_extraction_6mo_lambda\n",
    "```\n",
    "\n",
    "After staging/duplication the files, **inside the docker container built with the file Dockerfile.RDSNB**, run the Rscripts in a command like so : \n",
    "\n",
    "```\n",
    "find catdef_extraction_6mo/*.csv|grep -Piv 'preds'|awk '{ print\"./arima_train_6mo_cli.R \" $0   }' |parallel -j 6 --eta\n",
    "```\n",
    "\n",
    "Change the \"-j 6\" for parallel to be perhaps \"-j+0\" to use all the cores or the 6 to some higher or lower value depending on the number of cores available.  The command can also be run inside a \"screen\" so as to help ensure that the program's execution is not interrupted.\n",
    "\n",
    "Note that the arima_train_6mo_cli.R arima_train_6mo_lambda_cli.R scripts respectively are for analysis without and with 'lambda=\"auto\"' setting used in the auto.arima call.  To run both analysis, simply use the command above, but substitute the desired script.  To help observe the differnce between the two scripts (namely the lambda=auto), simply run \n",
    "\n",
    "```\n",
    "diff arima_train_6mo_cli.R arima_train_6mo_lambda_cli.R\n",
    "```\n",
    "\n",
    ".RData files are generated by the script which contain the arima models and predictions for each of the 24 test periods as well as a prediction for the \"25th\" test period (Jan 2017).  These can be loaded in R to examine the coefficients and other specifics of the individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bff838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc4daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
